#!/usr/bin/env python3
"""
ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬
data/documents í´ë”ì˜ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python upload_docs.py                           # ëª¨ë“  ë¬¸ì„œ ì—…ë¡œë“œ
    python upload_docs.py --folder policies/       # íŠ¹ì • í´ë”ë§Œ ì—…ë¡œë“œ
    python upload_docs.py --file guide.txt         # íŠ¹ì • íŒŒì¼ë§Œ ì—…ë¡œë“œ
    python upload_docs.py --incremental            # ì¦ë¶„ ì—…ë°ì´íŠ¸
    python upload_docs.py --batch-size 10          # ë°°ì¹˜ í¬ê¸° ì„¤ì •
"""

import asyncio
import os
import sys
import argparse
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Any, Set
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

class DocumentMetadata:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬"""
    
    def __init__(self, metadata_file: Path = None):
        self.metadata_file = metadata_file or Path("data/.doc_metadata.json")
        self.metadata = self.load_metadata()
    
    def load_metadata(self) -> Dict[str, Dict]:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception:
                pass
        return {}
    
    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        self.metadata_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
    
    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def is_file_changed(self, file_path: Path) -> bool:
        """íŒŒì¼ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        file_str = str(file_path)
        current_hash = self.get_file_hash(file_path)
        current_mtime = file_path.stat().st_mtime
        
        if file_str not in self.metadata:
            return True
        
        stored_data = self.metadata[file_str]
        return (stored_data.get('hash') != current_hash or 
                stored_data.get('mtime') != current_mtime)
    
    def update_file_metadata(self, file_path: Path):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        file_str = str(file_path)
        self.metadata[file_str] = {
            'hash': self.get_file_hash(file_path),
            'mtime': file_path.stat().st_mtime,
            'size': file_path.stat().st_size,
            'indexed_at': datetime.now().isoformat()
        }

def get_document_files(
    base_dir: Path, 
    folder: str = None, 
    file_path: str = None
) -> List[Path]:
    """ë¬¸ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    supported_extensions = {'.txt', '.md', '.docx', '.pdf'}
    
    if file_path:
        # íŠ¹ì • íŒŒì¼
        target_file = base_dir / file_path
        if target_file.exists() and target_file.suffix in supported_extensions:
            return [target_file]
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {target_file}")
            return []
    
    if folder:
        # íŠ¹ì • í´ë”
        target_dir = base_dir / folder
        if not target_dir.exists():
            print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_dir}")
            return []
        search_path = target_dir
    else:
        # ì „ì²´ ë¬¸ì„œ í´ë”
        search_path = base_dir
    
    # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰
    doc_files = []
    for ext in supported_extensions:
        doc_files.extend(search_path.rglob(f"*{ext}"))
    
    # íŒŒì¼ë§Œ í•„í„°ë§
    return [f for f in doc_files if f.is_file()]

async def upload_documents(
    folder: str = None,
    file_path: str = None,
    incremental: bool = False,
    batch_size: int = 5,
    force: bool = False
):
    """ë¬¸ì„œ ì—…ë¡œë“œ (ê³ ê¸‰ ì˜µì…˜ ì§€ì›)"""
    try:
        # Set environment variable for OpenAI API key
        from app.core.config import settings
        if settings.openai_api_key:
            os.environ['OPENAI_API_KEY'] = settings.openai_api_key
        elif settings.llm_api_key:
            os.environ['OPENAI_API_KEY'] = settings.llm_api_key
        
        from app.rag.enhanced_rag import rag_pipeline
        
        print("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘...")
        print(f"ë¬¸ì„œ í´ë”: {rag_pipeline.documents_dir}")
        
        # Create directory if not exists
        rag_pipeline.documents_dir.mkdir(parents=True, exist_ok=True)
        
        # Get document files
        doc_files = get_document_files(rag_pipeline.documents_dir, folder, file_path)
        
        if not doc_files:
            print("âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            if folder:
                print(f"í´ë” '{folder}'ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif file_path:
                print(f"íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"ë‹¤ìŒ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”: {rag_pipeline.documents_dir}")
            print("ì§€ì› í˜•ì‹: .txt, .md, .docx, .pdf")
            return
        
        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        metadata_manager = DocumentMetadata()
        files_to_process = doc_files
        
        if incremental and not force:
            print("ğŸ” ë³€ê²½ëœ íŒŒì¼ í™•ì¸ ì¤‘...")
            changed_files = []
            unchanged_files = []
            
            for doc_file in doc_files:
                if metadata_manager.is_file_changed(doc_file):
                    changed_files.append(doc_file)
                else:
                    unchanged_files.append(doc_file)
            
            files_to_process = changed_files
            
            print(f"   ğŸ“Š ì „ì²´ íŒŒì¼: {len(doc_files)}ê°œ")
            print(f"   ğŸ”„ ë³€ê²½ëœ íŒŒì¼: {len(changed_files)}ê°œ")
            print(f"   âœ… ë³€ê²½ ì—†ëŠ” íŒŒì¼: {len(unchanged_files)}ê°œ")
            
            if not changed_files:
                print("âœ… ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return
            
            print(f"\në³€ê²½ëœ íŒŒì¼ ëª©ë¡:")
            for f in changed_files:
                relative_path = f.relative_to(rag_pipeline.documents_dir)
                print(f"  - {relative_path}")
        else:
            print(f"\në°œê²¬ëœ ë¬¸ì„œ: {len(doc_files)}ê°œ")
            for f in doc_files[:10]:  # Show first 10
                relative_path = f.relative_to(rag_pipeline.documents_dir)
                print(f"  - {relative_path}")
            if len(doc_files) > 10:
                print(f"  ... ë° {len(doc_files) - 10}ê°œ ë”")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        print(f"\nğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")
        
        # For now, process all at once (could be enhanced for true batch processing)
        result = await rag_pipeline.load_documents()
        
        if result["success"]:
            print(f"\nâœ… ì—…ë¡œë“œ ì™„ë£Œ!")
            print(f"  - ì²˜ë¦¬ëœ íŒŒì¼: {len(result['processed_files'])}ê°œ")
            print(f"  - ìƒì„±ëœ ì²­í¬: {result['total_chunks']}ê°œ")
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if incremental:
                print("ğŸ“ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
                for doc_file in files_to_process:
                    metadata_manager.update_file_metadata(doc_file)
                metadata_manager.save_metadata()
                print("   âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ë¨")
            
            # Show stats
            stats = rag_pipeline.get_collection_stats()
            print(f"\nğŸ“Š ë²¡í„° DB í†µê³„:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
            print(f"  - ì²­í¬ í¬ê¸°: {stats['chunk_size']}")
            print(f"  - ì„ë² ë”© ëª¨ë¸: {stats['embedding_model']}")
            
            if result.get('errors'):
                print(f"\nâš ï¸  ì˜¤ë¥˜ ë°œìƒ: {len(result['errors'])}ê°œ")
                for error in result['errors'][:3]:
                    print(f"  - {error}")
                if len(result['errors']) > 3:
                    print(f"  ... ë° {len(result['errors']) - 3}ê°œ ë”")
        else:
            print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()

async def test_rag_query():
    """Test RAG query"""
    try:
        # Ensure API key is set
        from app.core.config import settings
        if settings.llm_api_key:
            os.environ['OPENAI_API_KEY'] = settings.llm_api_key
            
        from app.rag.enhanced_rag import rag_pipeline
        
        print("\n\nğŸ” RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
        print("-" * 50)
        
        # Test queries
        test_queries = [
            "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ì‚¬ìš©ëœ ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        for query in test_queries[:1]:  # Test with first query
            print(f"\nğŸ“ ì§ˆë¬¸: {query}")
            
            result = await rag_pipeline.answer_with_confidence(query, k=3)
            
            print(f"\nğŸ’¬ ë‹µë³€: {result['answer']}")
            print(f"ğŸ¯ ì‹ ë¢°ë„: {result['confidence']}")
            print(f"ğŸ’¡ ê·¼ê±°: {result['reasoning']}")
            
            if result['sources']:
                print(f"\nğŸ“š ì¶œì²˜:")
                for source in result['sources']:
                    print(f"  - {os.path.basename(source)}")
            
            if 'search_metadata' in result:
                print(f"\nğŸ” ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°:")
                print(f"  - ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(result['search_metadata'].get('rewritten_queries', []))}")
                print(f"  - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {result['search_metadata'].get('total_results', 0)}")
                
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="MOJI RAG ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python upload_docs.py                           # ëª¨ë“  ë¬¸ì„œ ì—…ë¡œë“œ
  python upload_docs.py --folder policies/       # íŠ¹ì • í´ë”ë§Œ ì—…ë¡œë“œ
  python upload_docs.py --file guide.txt         # íŠ¹ì • íŒŒì¼ë§Œ ì—…ë¡œë“œ
  python upload_docs.py --incremental            # ì¦ë¶„ ì—…ë°ì´íŠ¸
  python upload_docs.py --batch-size 10          # ë°°ì¹˜ í¬ê¸° ì„¤ì •
  python upload_docs.py --incremental --force    # ê°•ì œ ì „ì²´ ì¬ì¸ë±ì‹±
        """
    )
    
    parser.add_argument(
        '--folder',
        type=str,
        help='íŠ¹ì • í´ë”ë§Œ ì¸ë±ì‹± (ì˜ˆ: policies/)'
    )
    
    parser.add_argument(
        '--file',
        type=str,
        help='íŠ¹ì • íŒŒì¼ë§Œ ì¸ë±ì‹± (ì˜ˆ: guide.txt)'
    )
    
    parser.add_argument(
        '--incremental',
        action='store_true',
        help='ì¦ë¶„ ì—…ë°ì´íŠ¸ (ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=5,
        help='ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 5)'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='ê°•ì œ ì‹¤í–‰ (ì¦ë¶„ ëª¨ë“œì—ì„œë„ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬)'
    )
    
    parser.add_argument(
        '--no-test',
        action='store_true',
        help='í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ'
    )
    
    return parser.parse_args()

async def main():
    """Main function"""
    args = parse_arguments()
    
    print("ğŸ¤– MOJI RAG ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬")
    print("=" * 50)
    
    # ì„¤ì • ì •ë³´ ì¶œë ¥
    if args.folder:
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {args.folder}")
    elif args.file:
        print(f"ğŸ“„ ëŒ€ìƒ íŒŒì¼: {args.file}")
    else:
        print("ğŸ“ ëŒ€ìƒ: ì „ì²´ ë¬¸ì„œ í´ë”")
    
    if args.incremental:
        print("ğŸ”„ ëª¨ë“œ: ì¦ë¶„ ì—…ë°ì´íŠ¸")
        if args.force:
            print("âš¡ ê°•ì œ ëª¨ë“œ: ëª¨ë“  íŒŒì¼ ì¬ì²˜ë¦¬")
    else:
        print("ğŸ”„ ëª¨ë“œ: ì „ì²´ ì—…ë¡œë“œ")
    
    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print()
    
    # Upload documents
    await upload_documents(
        folder=args.folder,
        file_path=args.file,
        incremental=args.incremental,
        batch_size=args.batch_size,
        force=args.force
    )
    
    # Test query (unless disabled)
    if not args.no_test:
        response = input("\n\nRAG í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() == 'y':
            await test_rag_query()
    
    print("\n\nğŸ’¡ ì›¹ì±—ì—ì„œ RAG ì‚¬ìš©í•˜ê¸°:")
    print("1. ì„œë²„ ì‹¤í–‰: uvicorn app.main:app --reload")
    print("2. ì›¹ì±— ì ‘ì†: http://localhost:8000/static/moji-webchat-v2.htmll")
    print("3. ëª…ë ¹ì–´:")
    print("   - RAG í† ê¸€ì„ ONìœ¼ë¡œ ì„¤ì •")
    print("   - ì¼ë°˜ ì§ˆë¬¸ ì…ë ¥")
    print("   - /rag-help - ë„ì›€ë§")
    print("   - /rag-stats - í†µê³„ ë³´ê¸°")

if __name__ == "__main__":
    asyncio.run(main())