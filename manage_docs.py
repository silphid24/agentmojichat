#!/usr/bin/env python3
"""
MOJI RAG ë¬¸ì„œ ê´€ë¦¬ ë„êµ¬
ë¬¸ì„œ ì¸ë±ì‹±, ê²€ìƒ‰, í†µê³„, ë°±ì—…/ë³µì› ë“± í†µí•© ê´€ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import argparse
import json
import shutil
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

class DocumentManager:
    """í†µí•© ë¬¸ì„œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.setup_env()
        
    def setup_env(self):
        """í™˜ê²½ ì„¤ì •"""
        from app.core.config import settings
        if settings.openai_api_key:
            os.environ['OPENAI_API_KEY'] = settings.openai_api_key
        elif settings.llm_api_key:
            os.environ['OPENAI_API_KEY'] = settings.llm_api_key
    
    async def list_documents(self, folder: str = None, show_metadata: bool = False):
        """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print("ğŸ“š ë¬¸ì„œ ëª©ë¡")
            print("=" * 50)
            
            # ë¬¸ì„œ í´ë” í™•ì¸
            docs_dir = rag_pipeline.documents_dir
            if not docs_dir.exists():
                print(f"âŒ ë¬¸ì„œ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {docs_dir}")
                return
            
            # íŒŒì¼ ê²€ìƒ‰
            supported_extensions = {'.txt', '.md', '.docx', '.pdf'}
            search_path = docs_dir / folder if folder else docs_dir
            
            if not search_path.exists():
                print(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {search_path}")
                return
            
            doc_files = []
            for ext in supported_extensions:
                doc_files.extend(search_path.rglob(f"*{ext}"))
            
            doc_files = [f for f in doc_files if f.is_file()]
            doc_files.sort()
            
            if not doc_files:
                print("ğŸ“­ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"ğŸ“Š ì´ {len(doc_files)}ê°œ ë¬¸ì„œ")
            if folder:
                print(f"ğŸ“ í´ë”: {folder}")
            print()
            
            # íŒŒì¼ ëª©ë¡ ì¶œë ¥
            for i, doc_file in enumerate(doc_files, 1):
                relative_path = doc_file.relative_to(docs_dir)
                file_size = doc_file.stat().st_size
                file_mtime = datetime.fromtimestamp(doc_file.stat().st_mtime)
                
                print(f"{i:3d}. {relative_path}")
                print(f"     ğŸ“ í¬ê¸°: {file_size:,} bytes")
                print(f"     ğŸ“… ìˆ˜ì •: {file_mtime.strftime('%Y-%m-%d %H:%M:%S')}")
                
                if show_metadata:
                    # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶”ê°€
                    metadata_file = Path("data/.doc_metadata.json")
                    if metadata_file.exists():
                        try:
                            with open(metadata_file, 'r') as f:
                                metadata = json.load(f)
                            
                            file_str = str(doc_file)
                            if file_str in metadata:
                                meta = metadata[file_str]
                                indexed_at = meta.get('indexed_at', 'Unknown')
                                print(f"     ğŸ—‚ï¸  ì¸ë±ì‹±: {indexed_at}")
                        except Exception:
                            pass
                print()
                
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {str(e)}")
    
    async def search_documents(self, query: str, max_results: int = 5):
        """ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print(f"ğŸ” ë¬¸ì„œ ê²€ìƒ‰: '{query}'")
            print("=" * 50)
            
            result = await rag_pipeline.answer_with_confidence(query, k=max_results)
            
            if not result or not result.get('answer'):
                print("âŒ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                return
            
            print(f"ğŸ’¬ ë‹µë³€:")
            print(f"{result['answer']}")
            print()
            
            print(f"ğŸ¯ ì‹ ë¢°ë„: {result.get('confidence', 'Unknown')}")
            print(f"ğŸ’¡ ê·¼ê±°: {result.get('reasoning', 'None')}")
            print()
            
            if result.get('sources'):
                print(f"ğŸ“š ì¶œì²˜ ({len(result['sources'])}ê°œ):")
                for i, source in enumerate(result['sources'], 1):
                    print(f"  {i}. {os.path.basename(source)}")
                print()
            
            if result.get('search_metadata'):
                metadata = result['search_metadata']
                print(f"ğŸ” ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°:")
                print(f"  - ì¬ì‘ì„±ëœ ì¿¼ë¦¬: {len(metadata.get('rewritten_queries', []))}ê°œ")
                print(f"  - ê²€ìƒ‰ëœ ë¬¸ì„œ: {metadata.get('total_results', 0)}ê°œ")
                
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
    
    async def show_stats(self):
        """í†µê³„ ì •ë³´ í‘œì‹œ"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print("ğŸ“Š RAG ì‹œìŠ¤í…œ í†µê³„")
            print("=" * 50)
            
            # ê¸°ë³¸ í†µê³„
            stats = rag_pipeline.get_collection_stats()
            
            print(f"ğŸ“š ë²¡í„° DB ì •ë³´:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {stats.get('total_documents', 0)}")
            print(f"  - ì²­í¬ í¬ê¸°: {stats.get('chunk_size', 0)}")
            print(f"  - ì²­í¬ ì¤‘ë³µ: {stats.get('chunk_overlap', 0)}")
            print(f"  - ì„ë² ë”© ëª¨ë¸: {stats.get('embedding_model', 'Unknown')}")
            print()
            
            # íŒŒì¼ ì‹œìŠ¤í…œ í†µê³„
            docs_dir = rag_pipeline.documents_dir
            if docs_dir.exists():
                supported_extensions = {'.txt', '.md', '.docx', '.pdf'}
                file_counts = {}
                total_size = 0
                
                for ext in supported_extensions:
                    files = list(docs_dir.rglob(f"*{ext}"))
                    files = [f for f in files if f.is_file()]
                    file_counts[ext] = len(files)
                    
                    for file in files:
                        total_size += file.stat().st_size
                
                print(f"ğŸ“ íŒŒì¼ ì‹œìŠ¤í…œ ì •ë³´:")
                print(f"  - ë¬¸ì„œ í´ë”: {docs_dir}")
                print(f"  - ì´ íŒŒì¼ ìˆ˜: {sum(file_counts.values())}")
                for ext, count in file_counts.items():
                    if count > 0:
                        print(f"  - {ext} íŒŒì¼: {count}ê°œ")
                print(f"  - ì´ í¬ê¸°: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
                print()
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´
            metadata_file = Path("data/.doc_metadata.json")
            if metadata_file.exists():
                try:
                    with open(metadata_file, 'r') as f:
                        metadata = json.load(f)
                    
                    print(f"ğŸ—‚ï¸  ë©”íƒ€ë°ì´í„° ì •ë³´:")
                    print(f"  - ì¶”ì  ì¤‘ì¸ íŒŒì¼: {len(metadata)}ê°œ")
                    
                    if metadata:
                        indexed_files = [f for f in metadata.values() if 'indexed_at' in f]
                        print(f"  - ì¸ë±ì‹±ëœ íŒŒì¼: {len(indexed_files)}ê°œ")
                        
                        if indexed_files:
                            latest = max(indexed_files, key=lambda x: x['indexed_at'])
                            print(f"  - ìµœê·¼ ì¸ë±ì‹±: {latest['indexed_at']}")
                except Exception:
                    print(f"  - ë©”íƒ€ë°ì´í„° íŒŒì¼ ì½ê¸° ì˜¤ë¥˜")
            else:
                print(f"ğŸ—‚ï¸  ë©”íƒ€ë°ì´í„°: ì—†ìŒ")
                
        except Exception as e:
            print(f"âŒ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
    
    async def cleanup_orphaned(self):
        """ê³ ì•„ íŒŒì¼ ì •ë¦¬"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print("ğŸ§¹ ê³ ì•„ íŒŒì¼ ì •ë¦¬")
            print("=" * 50)
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì‚­ì œëœ íŒŒì¼ ì°¾ê¸°
            metadata_file = Path("data/.doc_metadata.json")
            if not metadata_file.exists():
                print("ğŸ“­ ë©”íƒ€ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            with open(metadata_file, 'r') as f:
                metadata = json.load(f)
            
            orphaned_files = []
            for file_path in list(metadata.keys()):
                if not Path(file_path).exists():
                    orphaned_files.append(file_path)
            
            if not orphaned_files:
                print("âœ… ê³ ì•„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            print(f"ğŸ—‘ï¸  ë°œê²¬ëœ ê³ ì•„ íŒŒì¼: {len(orphaned_files)}ê°œ")
            for file_path in orphaned_files:
                print(f"  - {file_path}")
            
            response = input(f"\në©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() == 'y':
                for file_path in orphaned_files:
                    del metadata[file_path]
                
                with open(metadata_file, 'w') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… {len(orphaned_files)}ê°œ ê³ ì•„ íŒŒì¼ì´ ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì •ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    async def backup_index(self, backup_path: str):
        """ì¸ë±ìŠ¤ ë°±ì—…"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print(f"ğŸ’¾ ì¸ë±ìŠ¤ ë°±ì—…: {backup_path}")
            print("=" * 50)
            
            backup_dir = Path(backup_path)
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # ë²¡í„° DB ë°±ì—…
            vectordb_dir = rag_pipeline.vectordb_dir
            if vectordb_dir.exists():
                backup_vectordb = backup_dir / "vectordb"
                if backup_vectordb.exists():
                    shutil.rmtree(backup_vectordb)
                shutil.copytree(vectordb_dir, backup_vectordb)
                print(f"âœ… ë²¡í„° DB ë°±ì—…ë¨: {backup_vectordb}")
            
            # ë©”íƒ€ë°ì´í„° ë°±ì—…
            metadata_file = Path("data/.doc_metadata.json")
            if metadata_file.exists():
                backup_metadata = backup_dir / "doc_metadata.json"
                shutil.copy2(metadata_file, backup_metadata)
                print(f"âœ… ë©”íƒ€ë°ì´í„° ë°±ì—…ë¨: {backup_metadata}")
            
            # ë°±ì—… ì •ë³´ ì €ì¥
            backup_info = {
                "created_at": datetime.now().isoformat(),
                "vectordb_included": vectordb_dir.exists(),
                "metadata_included": metadata_file.exists(),
                "stats": rag_pipeline.get_collection_stats()
            }
            
            backup_info_file = backup_dir / "backup_info.json"
            with open(backup_info_file, 'w') as f:
                json.dump(backup_info, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ë°±ì—… ì™„ë£Œ: {backup_dir}")
            
        except Exception as e:
            print(f"âŒ ë°±ì—… ì˜¤ë¥˜: {str(e)}")
    
    async def restore_index(self, backup_path: str):
        """ì¸ë±ìŠ¤ ë³µì›"""
        try:
            from app.rag.enhanced_rag import rag_pipeline
            
            print(f"ğŸ“‚ ì¸ë±ìŠ¤ ë³µì›: {backup_path}")
            print("=" * 50)
            
            backup_dir = Path(backup_path)
            if not backup_dir.exists():
                print(f"âŒ ë°±ì—… ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {backup_dir}")
                return
            
            # ë°±ì—… ì •ë³´ í™•ì¸
            backup_info_file = backup_dir / "backup_info.json"
            if backup_info_file.exists():
                with open(backup_info_file, 'r') as f:
                    backup_info = json.load(f)
                print(f"ğŸ“… ë°±ì—… ìƒì„±ì¼: {backup_info.get('created_at', 'Unknown')}")
                print()
            
            response = input("âš ï¸  í˜„ì¬ ì¸ë±ìŠ¤ë¥¼ ë®ì–´ì“°ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
            if response.lower() != 'y':
                print("âŒ ë³µì›ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return
            
            # ë²¡í„° DB ë³µì›
            backup_vectordb = backup_dir / "vectordb"
            if backup_vectordb.exists():
                vectordb_dir = rag_pipeline.vectordb_dir
                if vectordb_dir.exists():
                    shutil.rmtree(vectordb_dir)
                shutil.copytree(backup_vectordb, vectordb_dir)
                print(f"âœ… ë²¡í„° DB ë³µì›ë¨")
            
            # ë©”íƒ€ë°ì´í„° ë³µì›
            backup_metadata = backup_dir / "doc_metadata.json"
            if backup_metadata.exists():
                metadata_file = Path("data/.doc_metadata.json")
                metadata_file.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(backup_metadata, metadata_file)
                print(f"âœ… ë©”íƒ€ë°ì´í„° ë³µì›ë¨")
            
            print(f"âœ… ë³µì› ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ë³µì› ì˜¤ë¥˜: {str(e)}")

def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="MOJI RAG ë¬¸ì„œ ê´€ë¦¬ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # list ëª…ë ¹ì–´
    list_parser = subparsers.add_parser('list', help='ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ')
    list_parser.add_argument('--folder', type=str, help='íŠ¹ì • í´ë”ë§Œ ì¡°íšŒ')
    list_parser.add_argument('--metadata', action='store_true', help='ë©”íƒ€ë°ì´í„° ì •ë³´ í¬í•¨')
    
    # search ëª…ë ¹ì–´
    search_parser = subparsers.add_parser('search', help='ë¬¸ì„œ ê²€ìƒ‰')
    search_parser.add_argument('query', type=str, help='ê²€ìƒ‰ ì¿¼ë¦¬')
    search_parser.add_argument('--max-results', type=int, default=5, help='ìµœëŒ€ ê²°ê³¼ ìˆ˜')
    
    # stats ëª…ë ¹ì–´
    subparsers.add_parser('stats', help='í†µê³„ ì •ë³´ í‘œì‹œ')
    
    # cleanup ëª…ë ¹ì–´
    subparsers.add_parser('cleanup', help='ê³ ì•„ íŒŒì¼ ì •ë¦¬')
    
    # backup ëª…ë ¹ì–´
    backup_parser = subparsers.add_parser('backup', help='ì¸ë±ìŠ¤ ë°±ì—…')
    backup_parser.add_argument('path', type=str, help='ë°±ì—… ê²½ë¡œ')
    
    # restore ëª…ë ¹ì–´
    restore_parser = subparsers.add_parser('restore', help='ì¸ë±ìŠ¤ ë³µì›')
    restore_parser.add_argument('path', type=str, help='ë°±ì—… ê²½ë¡œ')
    
    return parser.parse_args()

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()
    
    if not args.command:
        print("âŒ ëª…ë ¹ì–´ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”. --helpë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")
        return
    
    manager = DocumentManager()
    
    print("ğŸ› ï¸  MOJI RAG ë¬¸ì„œ ê´€ë¦¬ ë„êµ¬")
    print("=" * 50)
    
    try:
        if args.command == 'list':
            await manager.list_documents(args.folder, args.metadata)
        
        elif args.command == 'search':
            await manager.search_documents(args.query, args.max_results)
        
        elif args.command == 'stats':
            await manager.show_stats()
        
        elif args.command == 'cleanup':
            await manager.cleanup_orphaned()
        
        elif args.command == 'backup':
            await manager.backup_index(args.path)
        
        elif args.command == 'restore':
            await manager.restore_index(args.path)
        
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {args.command}")
    
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())