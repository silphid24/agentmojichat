#!/usr/bin/env python3
"""
MOJI RAG ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ ë„êµ¬
RAG ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ê±´ê°• ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê³  ë¬¸ì œë¥¼ ê°ì§€í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import json
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class RAGHealthChecker:
    """RAG ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€ í´ë˜ìŠ¤"""

    def __init__(self):
        self.setup_env()
        self.results = {}

    def setup_env(self):
        """í™˜ê²½ ì„¤ì •"""
        from app.core.config import settings

        if settings.openai_api_key:
            os.environ["OPENAI_API_KEY"] = settings.openai_api_key
        elif settings.llm_api_key:
            os.environ["OPENAI_API_KEY"] = settings.llm_api_key

    def print_header(self, title: str):
        """ì„¹ì…˜ í—¤ë” ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ {title}")
        print("=" * 60)

    def print_check(self, name: str, status: str, details: str = ""):
        """ì²´í¬ ê²°ê³¼ ì¶œë ¥"""
        status_icon = {"PASS": "âœ…", "WARN": "âš ï¸ ", "FAIL": "âŒ"}.get(status, "â“")

        print(f"{status_icon} {name}: {status}")
        if details:
            print(f"   {details}")

    async def check_environment(self) -> Dict[str, Any]:
        """í™˜ê²½ ì„¤ì • ì ê²€"""
        self.print_header("í™˜ê²½ ì„¤ì • ì ê²€")

        checks = {}

        # API Key í™•ì¸
        api_key = os.environ.get("OPENAI_API_KEY")
        if api_key:
            self.print_check("OpenAI API Key", "PASS", f"í‚¤ ê¸¸ì´: {len(api_key)} ë¬¸ì")
            checks["api_key"] = True
        else:
            self.print_check("OpenAI API Key", "FAIL", "API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            checks["api_key"] = False

        # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
        required_packages = ["langchain", "chromadb", "openai", "sentence_transformers"]

        missing_packages = []
        for package in required_packages:
            try:
                __import__(package)
                self.print_check(f"Package: {package}", "PASS")
            except ImportError:
                self.print_check(f"Package: {package}", "FAIL", "ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                missing_packages.append(package)

        checks["packages"] = {
            "missing": missing_packages,
            "all_installed": len(missing_packages) == 0,
        }

        return checks

    async def check_directories(self) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ì ê²€"""
        self.print_header("ë””ë ‰í† ë¦¬ êµ¬ì¡° ì ê²€")

        try:
            from app.rag.enhanced_rag import rag_pipeline

            checks = {}

            # ë¬¸ì„œ ë””ë ‰í† ë¦¬
            docs_dir = rag_pipeline.documents_dir
            if docs_dir.exists():
                doc_count = len(list(docs_dir.rglob("*.*")))
                self.print_check(
                    "ë¬¸ì„œ ë””ë ‰í† ë¦¬", "PASS", f"{docs_dir} ({doc_count}ê°œ íŒŒì¼)"
                )
                checks["docs_dir"] = {"exists": True, "file_count": doc_count}
            else:
                self.print_check("ë¬¸ì„œ ë””ë ‰í† ë¦¬", "WARN", f"{docs_dir} ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                checks["docs_dir"] = {"exists": False, "file_count": 0}

            # ë²¡í„° DB ë””ë ‰í† ë¦¬
            vectordb_dir = rag_pipeline.vectordb_dir
            if vectordb_dir.exists():
                db_size = sum(
                    f.stat().st_size for f in vectordb_dir.rglob("*") if f.is_file()
                )
                self.print_check(
                    "ë²¡í„° DB ë””ë ‰í† ë¦¬",
                    "PASS",
                    f"{vectordb_dir} ({db_size/1024/1024:.1f} MB)",
                )
                checks["vectordb_dir"] = {
                    "exists": True,
                    "size_mb": db_size / 1024 / 1024,
                }
            else:
                self.print_check(
                    "ë²¡í„° DB ë””ë ‰í† ë¦¬", "WARN", f"{vectordb_dir} ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
                )
                checks["vectordb_dir"] = {"exists": False, "size_mb": 0}

            # ë©”íƒ€ë°ì´í„° íŒŒì¼
            metadata_file = Path("data/.doc_metadata.json")
            if metadata_file.exists():
                with open(metadata_file, "r") as f:
                    metadata = json.load(f)
                self.print_check(
                    "ë©”íƒ€ë°ì´í„° íŒŒì¼", "PASS", f"{len(metadata)}ê°œ íŒŒì¼ ì¶”ì  ì¤‘"
                )
                checks["metadata"] = {"exists": True, "tracked_files": len(metadata)}
            else:
                self.print_check("ë©”íƒ€ë°ì´í„° íŒŒì¼", "WARN", "ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
                checks["metadata"] = {"exists": False, "tracked_files": 0}

            return checks

        except Exception as e:
            self.print_check("ë””ë ‰í† ë¦¬ ì ê²€", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}

    async def check_vector_store(self) -> Dict[str, Any]:
        """ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ ì ê²€"""
        self.print_header("ë²¡í„° ìŠ¤í† ì–´ ìƒíƒœ ì ê²€")

        try:
            from app.rag.enhanced_rag import rag_pipeline

            checks = {}

            # ì»¬ë ‰ì…˜ í†µê³„
            stats = rag_pipeline.get_collection_stats()

            if "error" in stats:
                self.print_check("ë²¡í„° ìŠ¤í† ì–´ ì ‘ê·¼", "FAIL", stats["error"])
                checks["accessible"] = False
                return checks

            # ë¬¸ì„œ ìˆ˜ í™•ì¸
            doc_count = stats.get("total_documents", 0)
            if doc_count > 0:
                self.print_check("ì¸ë±ì‹±ëœ ë¬¸ì„œ", "PASS", f"{doc_count}ê°œ")
                checks["document_count"] = doc_count
            else:
                self.print_check("ì¸ë±ì‹±ëœ ë¬¸ì„œ", "WARN", "ì¸ë±ì‹±ëœ ë¬¸ì„œ ì—†ìŒ")
                checks["document_count"] = 0

            # ì„¤ì • ì •ë³´
            chunk_size = stats.get("chunk_size", 0)
            chunk_overlap = stats.get("chunk_overlap", 0)
            embedding_model = stats.get("embedding_model", "Unknown")
            use_semantic_chunking = stats.get("use_semantic_chunking", False)

            self.print_check(
                "ì²­í¬ ì„¤ì •", "PASS", f"í¬ê¸°: {chunk_size}, ì¤‘ë³µ: {chunk_overlap}"
            )
            self.print_check(
                "ì˜ë¯¸ë¡ ì  ì²­í‚¹", "PASS" if use_semantic_chunking else "INFO", 
                "í™œì„±í™”ë¨" if use_semantic_chunking else "ë¹„í™œì„±í™”ë¨"
            )
            self.print_check("ì„ë² ë”© ëª¨ë¸", "PASS", embedding_model)

            checks.update(
                {
                    "accessible": True,
                    "chunk_size": chunk_size,
                    "chunk_overlap": chunk_overlap,
                    "embedding_model": embedding_model,
                    "use_semantic_chunking": use_semantic_chunking,
                }
            )

            return checks

        except Exception as e:
            self.print_check("ë²¡í„° ìŠ¤í† ì–´ ì ê²€", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}

    async def check_search_performance(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì„±ëŠ¥ ì ê²€"""
        self.print_header("ê²€ìƒ‰ ì„±ëŠ¥ ì ê²€")

        try:
            from app.rag.enhanced_rag import rag_pipeline

            checks = {}
            test_queries = [
                "ì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€ìš”",
                "ì‚¬ìš©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            ]

            performance_results = []

            for query in test_queries:
                start_time = time.time()

                try:
                    # ë‹¨ìˆœ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                    results = rag_pipeline.vectorstore.similarity_search(query, k=3)
                    search_time = time.time() - start_time

                    result_count = len(results)

                    if search_time < 1.0:
                        status = "PASS"
                    elif search_time < 3.0:
                        status = "WARN"
                    else:
                        status = "FAIL"

                    self.print_check(
                        f"ê²€ìƒ‰: '{query[:20]}...'",
                        status,
                        f"{search_time:.2f}ì´ˆ, {result_count}ê°œ ê²°ê³¼",
                    )

                    performance_results.append(
                        {
                            "query": query,
                            "time": search_time,
                            "result_count": result_count,
                            "status": status,
                        }
                    )

                except Exception as e:
                    self.print_check(
                        f"ê²€ìƒ‰: '{query[:20]}...'", "FAIL", f"ì˜¤ë¥˜: {str(e)}"
                    )
                    performance_results.append(
                        {"query": query, "error": str(e), "status": "FAIL"}
                    )

            # ì „ì²´ ì„±ëŠ¥ í‰ê°€
            successful_searches = [r for r in performance_results if "time" in r]
            if successful_searches:
                avg_time = sum(r["time"] for r in successful_searches) / len(
                    successful_searches
                )
                checks["average_search_time"] = avg_time
                checks["successful_searches"] = len(successful_searches)
                checks["total_searches"] = len(test_queries)

                self.print_check("ì „ì²´ ê²€ìƒ‰ ì„±ëŠ¥", "PASS", f"í‰ê·  {avg_time:.2f}ì´ˆ")
            else:
                checks["average_search_time"] = None
                checks["successful_searches"] = 0
                checks["total_searches"] = len(test_queries)

                self.print_check("ì „ì²´ ê²€ìƒ‰ ì„±ëŠ¥", "FAIL", "ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨")

            checks["detailed_results"] = performance_results
            return checks

        except Exception as e:
            self.print_check("ê²€ìƒ‰ ì„±ëŠ¥ ì ê²€", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}

    async def check_rag_pipeline(self) -> Dict[str, Any]:
        """RAG íŒŒì´í”„ë¼ì¸ ì ê²€"""
        self.print_header("RAG íŒŒì´í”„ë¼ì¸ ì ê²€")

        try:
            from app.rag.enhanced_rag import rag_pipeline

            checks = {}

            # ë‹¨ìˆœ RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            test_query = "ì‹œìŠ¤í…œì— ëŒ€í•´ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”"

            start_time = time.time()
            # Use the new adaptive method
            result = await rag_pipeline.generate_answer_adaptive(test_query)
            total_time = time.time() - start_time

            if result and result.get("answer"):
                answer_length = len(result["answer"])
                confidence = result.get("confidence", "Unknown")
                source_count = len(result.get("sources", []))

                self.print_check("RAG ì¿¼ë¦¬ ì‹¤í–‰", "PASS", f"{total_time:.2f}ì´ˆ")
                self.print_check("ë‹µë³€ ìƒì„±", "PASS", f"{answer_length} ë¬¸ì")
                self.print_check("ì‹ ë¢°ë„ í‰ê°€", "PASS", confidence)
                self.print_check("ì¶œì²˜ ì°¸ì¡°", "PASS", f"{source_count}ê°œ ì¶œì²˜")

                checks.update(
                    {
                        "query_successful": True,
                        "response_time": total_time,
                        "answer_length": answer_length,
                        "confidence": confidence,
                        "source_count": source_count,
                    }
                )

                # ìƒì„¸ ê²°ê³¼
                if "search_metadata" in result:
                    metadata = result["search_metadata"]
                    rewritten_queries = len(metadata.get("rewritten_queries", []))
                    total_results = metadata.get("total_results", 0)

                    self.print_check(
                        "ì¿¼ë¦¬ ì¬ì‘ì„±", "PASS", f"{rewritten_queries}ê°œ ë³€í˜•"
                    )
                    self.print_check("ê²€ìƒ‰ ê²°ê³¼", "PASS", f"{total_results}ê°œ í›„ë³´")

                    checks.update(
                        {
                            "rewritten_queries": rewritten_queries,
                            "search_results": total_results,
                        }
                    )

            else:
                self.print_check("RAG ì¿¼ë¦¬ ì‹¤í–‰", "FAIL", "ì‘ë‹µ ì—†ìŒ")
                checks["query_successful"] = False

                if "error" in result:
                    checks["error"] = result["error"]

            return checks

        except Exception as e:
            self.print_check("RAG íŒŒì´í”„ë¼ì¸ ì ê²€", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}

    async def check_llm_integration(self) -> Dict[str, Any]:
        """LLM í†µí•© ì ê²€"""
        self.print_header("LLM í†µí•© ì ê²€")

        try:
            from app.llm.router import llm_router

            checks = {}

            # LLM ë¼ìš°í„° ìƒíƒœ
            current_provider = llm_router.current_provider
            current_model = llm_router.current_model

            self.print_check("LLM ë¼ìš°í„°", "PASS", f"í”„ë¡œë°”ì´ë”: {current_provider}")
            self.print_check("í˜„ì¬ ëª¨ë¸", "PASS", current_model or "ê¸°ë³¸ê°’")

            # LangChain ëª¨ë¸ í…ŒìŠ¤íŠ¸
            try:
                llm = await llm_router.get_langchain_model()
                self.print_check("LangChain í†µí•©", "PASS", "ëª¨ë¸ ë¡œë“œë¨")
                checks["langchain_model"] = True
            except Exception as e:
                self.print_check("LangChain í†µí•©", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
                checks["langchain_model"] = False
                checks["langchain_error"] = str(e)

            checks.update(
                {"current_provider": current_provider, "current_model": current_model}
            )

            return checks

        except Exception as e:
            self.print_check("LLM í†µí•© ì ê²€", "FAIL", f"ì˜¤ë¥˜: {str(e)}")
            return {"error": str(e)}

    async def generate_recommendations(self, all_results: Dict[str, Any]) -> List[str]:
        """ì ê²€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        # í™˜ê²½ ì„¤ì • ë¬¸ì œ
        env_checks = all_results.get("environment", {})
        if not env_checks.get("api_key"):
            recommendations.append(
                "ğŸ”‘ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš” (.env íŒŒì¼ì˜ OPENAI_API_KEY)"
            )

        if env_checks.get("packages", {}).get("missing"):
            missing = env_checks["packages"]["missing"]
            recommendations.append(
                f"ğŸ“¦ ëˆ„ë½ëœ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”: pip install {' '.join(missing)}"
            )

        # ë””ë ‰í† ë¦¬ ë¬¸ì œ
        dir_checks = all_results.get("directories", {})
        if not dir_checks.get("docs_dir", {}).get("exists"):
            recommendations.append(
                "ğŸ“ ë¬¸ì„œ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•˜ê³  ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”: mkdir -p data/documents"
            )

        if dir_checks.get("docs_dir", {}).get("file_count", 0) == 0:
            recommendations.append(
                "ğŸ“„ data/documents/ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•œ í›„ python upload_docs.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”"
            )

        # ë²¡í„° ìŠ¤í† ì–´ ë¬¸ì œ
        vector_checks = all_results.get("vector_store", {})
        if vector_checks.get("document_count", 0) == 0:
            recommendations.append("ğŸ”„ ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ì„¸ìš”: python upload_docs.py")

        # ì„±ëŠ¥ ë¬¸ì œ
        perf_checks = all_results.get("search_performance", {})
        avg_time = perf_checks.get("average_search_time")
        if avg_time and avg_time > 3.0:  # Updated threshold for quality-first approach
            recommendations.append(
                "âš¡ ê²€ìƒ‰ ì„±ëŠ¥ì´ ëŠë¦½ë‹ˆë‹¤. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ë‚˜ ì²­í¬ í¬ê¸°ë¥¼ ì¡°ì •í•˜ì„¸ìš”"
            )
        
        # ì˜ë¯¸ë¡ ì  ì²­í‚¹ ê¶Œì¥
        vector_checks = all_results.get("vector_store", {})
        if not vector_checks.get("use_semantic_chunking"):
            recommendations.append(
                "ğŸ”§ ì˜ë¯¸ë¡ ì  ì²­í‚¹ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ í™œì„±í™”ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤"
            )

        # RAG íŒŒì´í”„ë¼ì¸ ë¬¸ì œ
        rag_checks = all_results.get("rag_pipeline", {})
        if not rag_checks.get("query_successful"):
            recommendations.append(
                "ğŸ”§ RAG íŒŒì´í”„ë¼ì¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”"
            )

        # LLM í†µí•© ë¬¸ì œ
        llm_checks = all_results.get("llm_integration", {})
        if not llm_checks.get("langchain_model"):
            recommendations.append(
                "ğŸ¤– LLM í†µí•©ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. API í‚¤ì™€ í”„ë¡œë°”ì´ë” ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”"
            )

        if not recommendations:
            recommendations.append("âœ… ëª¨ë“  ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!")

        return recommendations

    async def run_full_check(self, verbose: bool = True) -> Dict[str, Any]:
        """ì „ì²´ ìƒíƒœ ì ê²€ ì‹¤í–‰"""
        if verbose:
            print("ğŸ¥ MOJI RAG ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€")
            print("=" * 60)
            print(f"ğŸ“… ì ê²€ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        results = {}

        # ìˆœì°¨ì ìœ¼ë¡œ ì ê²€ ì‹¤í–‰
        try:
            results["environment"] = await self.check_environment()
            results["directories"] = await self.check_directories()
            results["vector_store"] = await self.check_vector_store()
            results["search_performance"] = await self.check_search_performance()
            results["rag_pipeline"] = await self.check_rag_pipeline()
            results["llm_integration"] = await self.check_llm_integration()

            # ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = await self.generate_recommendations(results)

            if verbose:
                self.print_header("ì¶”ì²œì‚¬í•­")
                for i, rec in enumerate(recommendations, 1):
                    print(f"{i}. {rec}")

                self.print_header("ìš”ì•½")

                # ì „ì²´ ìƒíƒœ ìš”ì•½
                total_checks = 0
                passed_checks = 0

                for category, checks in results.items():
                    if isinstance(checks, dict) and "error" not in checks:
                        for key, value in checks.items():
                            if isinstance(value, bool):
                                total_checks += 1
                                if value:
                                    passed_checks += 1

                success_rate = (
                    (passed_checks / total_checks * 100) if total_checks > 0 else 0
                )

                print(
                    f"ğŸ“Š ì „ì²´ ì ê²€: {passed_checks}/{total_checks} í†µê³¼ ({success_rate:.1f}%)"
                )

                if success_rate >= 90:
                    print("ğŸŸ¢ ì‹œìŠ¤í…œ ìƒíƒœ: ìš°ìˆ˜")
                elif success_rate >= 70:
                    print("ğŸŸ¡ ì‹œìŠ¤í…œ ìƒíƒœ: ì–‘í˜¸ (ì¼ë¶€ ê°œì„  í•„ìš”)")
                else:
                    print("ğŸ”´ ì‹œìŠ¤í…œ ìƒíƒœ: ì£¼ì˜ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)")

                print(f"ğŸ“… ì ê²€ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

            results["recommendations"] = recommendations
            results["summary"] = {
                "total_checks": total_checks,
                "passed_checks": passed_checks,
                "success_rate": success_rate,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            if verbose:
                print(f"âŒ ì ê²€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            results["error"] = str(e)

        return results


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="MOJI RAG ì‹œìŠ¤í…œ ìƒíƒœ ì ê²€")
    parser.add_argument("--json", action="store_true", help="JSON í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ ì¶œë ¥")
    parser.add_argument("--save", type=str, help="ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥")
    parser.add_argument("--quiet", action="store_true", help="ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”")

    args = parser.parse_args()

    checker = RAGHealthChecker()

    try:
        results = await checker.run_full_check(verbose=not args.quiet)

        if args.json or args.save:
            output = json.dumps(results, ensure_ascii=False, indent=2)

            if args.save:
                with open(args.save, "w", encoding="utf-8") as f:
                    f.write(output)
                print(f"\nğŸ“„ ê²°ê³¼ê°€ ì €ì¥ë¨: {args.save}")

            if args.json:
                print("\n" + output)

    except Exception as e:
        print(f"âŒ ì ê²€ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
