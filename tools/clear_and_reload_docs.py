#!/usr/bin/env python3
"""
ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì¬ì¸ë±ì‹± ë„êµ¬
ëª¨ë“  ê¸°ì¡´ ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•˜ê³  ë¬¸ì„œë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì¸ë±ì‹±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
  python clear_and_reload_docs.py              # ì „ì²´ ì´ˆê¸°í™” ë° ì¬ì¸ë±ì‹±
  python clear_and_reload_docs.py --clear-only # ë²¡í„° ìŠ¤í† ì–´ë§Œ ì´ˆê¸°í™” (ë¬¸ì„œ ì¬ì¸ë±ì‹± ì—†ì´)
"""

import asyncio
import os
import sys
import shutil
import argparse
from pathlib import Path
from typing import List, Dict, Any

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


async def clear_vector_store(include_faiss: bool = True):
    """ë²¡í„° ìŠ¤í† ì–´ ì™„ì „ ì´ˆê¸°í™”"""
    try:
        from app.rag.enhanced_rag import rag_pipeline

        print("ğŸ—‘ï¸  ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ ì¤‘...")

        # Remove ChromaDB vector database directory
        if rag_pipeline.vectordb_dir.exists():
            shutil.rmtree(rag_pipeline.vectordb_dir)
            print(f"   âœ… ChromaDB ì‚­ì œë¨: {rag_pipeline.vectordb_dir}")
        else:
            print(f"   â„¹ï¸  ChromaDB ì´ë¯¸ ë¹„ì–´ìˆìŒ: {rag_pipeline.vectordb_dir}")

        # Remove FAISS index if requested
        if include_faiss:
            faiss_index_dir = rag_pipeline.vectordb_dir.parent / "faiss_index"
            vector_index_dir = Path("data/vector_index")

            for faiss_dir in [faiss_index_dir, vector_index_dir]:
                if faiss_dir.exists():
                    shutil.rmtree(faiss_dir)
                    print(f"   âœ… FAISS ì¸ë±ìŠ¤ ì‚­ì œë¨: {faiss_dir}")
                else:
                    print(f"   â„¹ï¸  FAISS ì¸ë±ìŠ¤ ì´ë¯¸ ë¹„ì–´ìˆìŒ: {faiss_dir}")

        # Recreate ChromaDB directory
        rag_pipeline.vectordb_dir.mkdir(parents=True, exist_ok=True)
        print(f"   âœ… ChromaDB ë””ë ‰í† ë¦¬ ì¬ìƒì„±: {rag_pipeline.vectordb_dir}")

        return True

    except Exception as e:
        print(f"âŒ ë²¡í„° ìŠ¤í† ì–´ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        return False


async def scan_documents() -> List[Path]:
    """ë¬¸ì„œ í´ë” ìŠ¤ìº”"""
    try:
        from app.rag.enhanced_rag import rag_pipeline

        print(f"ğŸ“ ë¬¸ì„œ í´ë” ìŠ¤ìº”: {rag_pipeline.documents_dir}")

        # Create directory if not exists
        rag_pipeline.documents_dir.mkdir(parents=True, exist_ok=True)

        # Supported file extensions
        supported_extensions = {".txt", ".md", ".docx", ".pdf"}

        # Find all documents recursively
        doc_files = []
        for ext in supported_extensions:
            doc_files.extend(rag_pipeline.documents_dir.rglob(f"*{ext}"))

        # Filter out non-files
        doc_files = [f for f in doc_files if f.is_file()]

        print(f"   ğŸ“Š ë°œê²¬ëœ ë¬¸ì„œ: {len(doc_files)}ê°œ")

        # Group by extension
        by_ext = {}
        for doc in doc_files:
            ext = doc.suffix
            if ext not in by_ext:
                by_ext[ext] = []
            by_ext[ext].append(doc)

        for ext, files in by_ext.items():
            print(f"   {ext}: {len(files)}ê°œ")

        return doc_files

    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ìŠ¤ìº” ì‹¤íŒ¨: {str(e)}")
        return []


async def reload_all_documents(doc_files: List[Path]) -> Dict[str, Any]:
    """ëª¨ë“  ë¬¸ì„œ ì¬ì¸ë±ì‹±"""
    try:
        # Set environment variable for OpenAI API key
        from app.core.config import settings

        if settings.openai_api_key:
            os.environ["OPENAI_API_KEY"] = settings.openai_api_key
        elif settings.llm_api_key:
            os.environ["OPENAI_API_KEY"] = settings.llm_api_key

        from app.rag.enhanced_rag import rag_pipeline

        print("\nğŸ”„ ë¬¸ì„œ ì¬ì¸ë±ì‹± ì‹œì‘...")
        print(f"   ğŸ“ ì´ {len(doc_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì˜ˆì •")

        # Progress tracking
        processed_count = 0
        error_count = 0
        total_chunks = 0

        # Process documents
        result = await rag_pipeline.load_documents()

        if result["success"]:
            processed_count = len(result.get("processed_files", []))
            total_chunks = result.get("total_chunks", 0)

            print("\nâœ… ì¬ì¸ë±ì‹± ì™„ë£Œ!")
            print(f"   ğŸ“„ ì²˜ë¦¬ëœ íŒŒì¼: {processed_count}ê°œ")
            print(f"   ğŸ“ ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")

            if result.get("errors"):
                error_count = len(result["errors"])
                print(f"   âš ï¸  ì˜¤ë¥˜ ë°œìƒ: {error_count}ê°œ")
                for error in result["errors"][:3]:  # Show first 3 errors
                    print(f"      - {error}")
                if len(result["errors"]) > 3:
                    print(f"      ... ë° {len(result['errors']) - 3}ê°œ ë”")
        else:
            print(f"âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            return result

        # Get final stats
        stats = rag_pipeline.get_collection_stats()
        print("\nğŸ“Š ìµœì¢… í†µê³„:")
        print(f"   ğŸ“š ì´ ë¬¸ì„œ ìˆ˜: {stats.get('total_documents', 0)}")
        print(f"   ğŸ“¦ ì²­í¬ í¬ê¸°: {stats.get('chunk_size', 0)}")
        print(f"   ğŸ”„ ì²­í¬ ì¤‘ë³µ: {stats.get('chunk_overlap', 0)}")
        print(f"   ğŸ§© ì˜ë¯¸ë¡ ì  ì²­í‚¹: {'í™œì„±í™”' if stats.get('use_semantic_chunking') else 'ë¹„í™œì„±í™”'}")
        print(f"   ğŸ¤– ì„ë² ë”© ëª¨ë¸: {stats.get('embedding_model', 'Unknown')}")

        return {
            "success": True,
            "processed_files": processed_count,
            "total_chunks": total_chunks,
            "errors": error_count,
            "stats": stats,
        }

    except Exception as e:
        print(f"âŒ ì¬ì¸ë±ì‹± ì‹¤íŒ¨: {str(e)}")
        import traceback

        traceback.print_exc()
        return {"success": False, "error": str(e)}


async def verify_indexing():
    """ì¸ë±ì‹± ê²°ê³¼ ê²€ì¦"""
    try:
        from app.rag.enhanced_rag import rag_pipeline

        print("\nğŸ” ì¸ë±ì‹± ê²°ê³¼ ê²€ì¦...")

        # Test query
        test_query = "ì‹œìŠ¤í…œì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"

        print(f"   ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")

        result = await rag_pipeline.answer_with_confidence(test_query, k=3)

        if result and result.get("answer"):
            print("   âœ… ì¿¼ë¦¬ ì„±ê³µ")
            print(f"   ğŸ¯ ì‹ ë¢°ë„: {result.get('confidence', 'Unknown')}")
            print(f"   ğŸ“š ì¶œì²˜: {len(result.get('sources', []))}ê°œ")

            if result.get("sources"):
                print("   ğŸ“„ ì¶œì²˜ íŒŒì¼:")
                for source in result["sources"][:3]:
                    print(f"      - {os.path.basename(source)}")
        else:
            print("   âš ï¸  ì¿¼ë¦¬ ì‘ë‹µ ì—†ìŒ (ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì„± ë‚®ìŒ)")

        return True

    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        return False


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¬¸ì„œ ì¬ì¸ë±ì‹± ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python clear_and_reload_docs.py              # ì „ì²´ ì´ˆê¸°í™” ë° ì¬ì¸ë±ì‹±
  python clear_and_reload_docs.py -c # ë²¡í„° ìŠ¤í† ì–´ë§Œ ì´ˆê¸°í™”
  python clear_and_reload_docs.py -y           # í™•ì¸ ì—†ì´ ì „ì²´ ì‘ì—… ì‹¤í–‰
  python clear_and_reload_docs.py --clear-only -y  # í™•ì¸ ì—†ì´ ì´ˆê¸°í™”ë§Œ ì‹¤í–‰
        """,
    )

    parser.add_argument(
        "-c", "--clear-only",
        action="store_true",
        help="ë²¡í„° ìŠ¤í† ì–´ë§Œ ì´ˆê¸°í™”í•˜ê³  ë¬¸ì„œ ì¬ì¸ë±ì‹±ì€ ê±´ë„ˆë›°ê¸°",
    )

    parser.add_argument(
        "-y", "--yes", action="store_true", help="í™•ì¸ í”„ë¡¬í”„íŠ¸ ì—†ì´ ìë™ìœ¼ë¡œ ì§„í–‰"
    )

    parser.add_argument(
        "--no-faiss",
        action="store_true",
        help="FAISS ì¸ë±ìŠ¤ëŠ” ì‚­ì œí•˜ì§€ ì•Šê³  ChromaDBë§Œ ì´ˆê¸°í™”",
    )

    return parser.parse_args()


async def clear_only_mode():
    """ë²¡í„° ìŠ¤í† ì–´ë§Œ ì´ˆê¸°í™”í•˜ëŠ” ëª¨ë“œ"""
    print("ğŸ§¹ MOJI RAG ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” (ì´ˆê¸°í™”ë§Œ)")
    print("=" * 60)

    try:
        # Get args to check FAISS deletion option
        args = parse_arguments()
        include_faiss = not args.no_faiss

        print("\nğŸ“ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹œì‘")
        if not await clear_vector_store(include_faiss=include_faiss):
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return False

        print("\nâœ… ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì™„ë£Œ!")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ë¬¸ì„œ ì¬ì¸ë±ì‹±: python clear_and_reload_docs.py")
        print("   2. ë˜ëŠ” ì„œë²„ì—ì„œ ë¬¸ì„œ ìë™ ë¡œë“œ ëŒ€ê¸°")

        return True

    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    args = parse_arguments()

    if args.clear_only:
        await clear_only_mode()
        return

    print("ğŸ§¹ MOJI RAG ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ì¬ì¸ë±ì‹±")
    print("=" * 60)

    # Warning message (skip if --yes flag is used)
    if not args.yes:
        print("âš ï¸  ê²½ê³ : ì´ ì‘ì—…ì€ ê¸°ì¡´ì˜ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤!")
        response = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")

        if response.lower() != "y":
            print("âŒ ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
    else:
        print("âš¡ ìë™ ì‹¤í–‰ ëª¨ë“œ (--yes í”Œë˜ê·¸ ì‚¬ìš©)")

    start_time = asyncio.get_event_loop().time()

    try:
        # Step 1: Clear vector store
        print("\nğŸ“ 1ë‹¨ê³„: ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”")
        include_faiss = not args.no_faiss
        if not await clear_vector_store(include_faiss=include_faiss):
            print("âŒ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return

        # Step 2: Scan documents
        print("\nğŸ“ 2ë‹¨ê³„: ë¬¸ì„œ ìŠ¤ìº”")
        doc_files = await scan_documents()

        if not doc_files:
            print("âš ï¸  ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            print("   data/documents/ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
            return

        # Step 3: Reload documents
        print("\nğŸ“ 3ë‹¨ê³„: ë¬¸ì„œ ì¬ì¸ë±ì‹±")
        result = await reload_all_documents(doc_files)

        if not result.get("success"):
            print("âŒ ë¬¸ì„œ ì¬ì¸ë±ì‹± ì‹¤íŒ¨")
            return

        # Step 4: Verify
        print("\nğŸ“ 4ë‹¨ê³„: ê²€ì¦")
        await verify_indexing()

        # Summary
        end_time = asyncio.get_event_loop().time()
        duration = end_time - start_time

        print("\nğŸ‰ ì¬ì¸ë±ì‹± ì™„ë£Œ!")
        print(f"   â±ï¸  ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"   ğŸ“„ ì²˜ë¦¬ëœ íŒŒì¼: {result.get('processed_files', 0)}ê°œ")
        print(f"   ğŸ“ ìƒì„±ëœ ì²­í¬: {result.get('total_chunks', 0)}ê°œ")

        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ì„œë²„ ì‹œì‘: uvicorn app.main:app --reload")
        print("   2. ì›¹ì±— ì ‘ì†: http://localhost:8001/static/moji-webchat-v2.html")
        print("   3. RAG í…ŒìŠ¤íŠ¸: í† ê¸€ì„ ONìœ¼ë¡œ ì„¤ì •í•˜ê³  ì§ˆë¬¸í•˜ê¸°")

    except Exception as e:
        print(f"\nâŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
