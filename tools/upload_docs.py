#!/usr/bin/env python3
"""
ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬
data/documents í´ë”ì˜ ë¬¸ì„œë¥¼ ë²¡í„° DBì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python upload_docs.py                           # ëª¨ë“  ë¬¸ì„œ ì—…ë¡œë“œ
    python upload_docs.py --folder policies/       # íŠ¹ì • í´ë”ë§Œ ì—…ë¡œë“œ
    python upload_docs.py --file guide.txt         # íŠ¹ì • íŒŒì¼ë§Œ ì—…ë¡œë“œ
    python upload_docs.py --incremental            # ì¦ë¶„ ì—…ë°ì´íŠ¸
    python upload_docs.py --batch-size 10          # ë°°ì¹˜ í¬ê¸° ì„¤ì •
"""

import asyncio
import os
import sys
import argparse
import hashlib
import json
from pathlib import Path
from typing import List, Dict
from datetime import datetime

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


class DocumentMetadata:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬"""

    def __init__(self, metadata_file: Path = None):
        self.metadata_file = metadata_file or Path("data/.doc_metadata.json")
        self.metadata = self.load_metadata()

    def load_metadata(self) -> Dict[str, Dict]:
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                pass
        return {}

    def save_metadata(self):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        self.metadata_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.metadata_file, "w", encoding="utf-8") as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)

    def get_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def is_file_changed(self, file_path: Path) -> bool:
        """íŒŒì¼ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        file_str = str(file_path)
        current_hash = self.get_file_hash(file_path)
        current_mtime = file_path.stat().st_mtime

        if file_str not in self.metadata:
            return True

        stored_data = self.metadata[file_str]
        return (
            stored_data.get("hash") != current_hash
            or stored_data.get("mtime") != current_mtime
        )

    def update_file_metadata(self, file_path: Path):
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        file_str = str(file_path)
        self.metadata[file_str] = {
            "hash": self.get_file_hash(file_path),
            "mtime": file_path.stat().st_mtime,
            "size": file_path.stat().st_size,
            "indexed_at": datetime.now().isoformat(),
        }


def get_document_files(
    base_dir: Path, folder: str = None, file_path: str = None
) -> List[Path]:
    """ë¬¸ì„œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    supported_extensions = {".txt", ".md", ".docx", ".pdf"}

    if file_path:
        # íŠ¹ì • íŒŒì¼
        target_file = base_dir / file_path
        if target_file.exists() and target_file.suffix in supported_extensions:
            if not _is_excluded_file(target_file):
                return [target_file]
        else:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹: {target_file}")
            return []

    if folder:
        # íŠ¹ì • í´ë”
        target_dir = base_dir / folder
        if not target_dir.exists():
            print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_dir}")
            return []
        search_path = target_dir
    else:
        # ì „ì²´ ë¬¸ì„œ í´ë”
        search_path = base_dir

    # ì¬ê·€ì ìœ¼ë¡œ íŒŒì¼ ê²€ìƒ‰
    doc_files = []
    for ext in supported_extensions:
        doc_files.extend(search_path.rglob(f"*{ext}"))

    # íŒŒì¼ í•„í„°ë§ (ì¡´ì¬í•˜ê³ , ì œì™¸ ëª©ë¡ì— ì—†ëŠ” ê²ƒë§Œ)
    filtered_files = []
    for f in doc_files:
        if f.is_file() and not _is_excluded_file(f):
            # íŒŒì¼ í¬ê¸° ì²´í¬ (ë„ˆë¬´ ì‘ê±°ë‚˜ í° íŒŒì¼ ì œì™¸)
            try:
                file_size = f.stat().st_size
                if file_size < 10:  # 10ë°”ì´íŠ¸ ë¯¸ë§Œ íŒŒì¼ ì œì™¸
                    print(f"âš ï¸  íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŒ (ì œì™¸): {f.name} ({file_size} bytes)")
                    continue
                if file_size > 10 * 1024 * 1024:  # 10MB ì´ˆê³¼ íŒŒì¼ ì œì™¸
                    print(
                        f"âš ï¸  íŒŒì¼ì´ ë„ˆë¬´ í¼ (ì œì™¸): {f.name} ({file_size / 1024 / 1024:.1f} MB)"
                    )
                    continue
                filtered_files.append(f)
            except Exception as e:
                print(f"âš ï¸  íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ (ì œì™¸): {f.name} - {e}")
                continue

    return filtered_files


def _is_excluded_file(file_path: Path) -> bool:
    """ì œì™¸í•  íŒŒì¼ì¸ì§€ í™•ì¸"""
    filename = file_path.name

    # Word ì„ì‹œ íŒŒì¼ ì œì™¸
    if filename.startswith("~$"):
        return True

    # ìˆ¨ê¹€ íŒŒì¼ ì œì™¸
    if filename.startswith("."):
        return True

    # ë°±ì—… íŒŒì¼ ì œì™¸
    if filename.endswith(".bak") or filename.endswith(".backup"):
        return True

    # ë¹ˆ íŒŒì¼ëª… ì œì™¸
    if not filename.strip():
        return True

    return False


async def upload_documents(
    folder: str = None,
    file_path: str = None,
    incremental: bool = False,
    batch_size: int = 5,
    force: bool = False,
):
    """ë¬¸ì„œ ì—…ë¡œë“œ (ê³ ê¸‰ ì˜µì…˜ ì§€ì›)"""
    try:
        # Set environment variable for OpenAI API key
        from app.core.config import settings

        if settings.openai_api_key:
            os.environ["OPENAI_API_KEY"] = settings.openai_api_key
        elif settings.llm_api_key:
            os.environ["OPENAI_API_KEY"] = settings.llm_api_key

        from app.rag.enhanced_rag import rag_pipeline

        print("ğŸ“ ë¬¸ì„œ ì—…ë¡œë“œ ì‹œì‘...")
        print(f"ë¬¸ì„œ í´ë”: {rag_pipeline.documents_dir}")

        # Create directory if not exists
        rag_pipeline.documents_dir.mkdir(parents=True, exist_ok=True)

        # Get document files
        doc_files = get_document_files(rag_pipeline.documents_dir, folder, file_path)

        if not doc_files:
            print("âš ï¸  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤!")
            if folder:
                print(f"í´ë” '{folder}'ì—ì„œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif file_path:
                print(f"íŒŒì¼ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"ë‹¤ìŒ í´ë”ì— ë¬¸ì„œë¥¼ ì¶”ê°€í•˜ì„¸ìš”: {rag_pipeline.documents_dir}")
            print("ì§€ì› í˜•ì‹: .txt, .md, .docx, .pdf")
            return

        # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
        metadata_manager = DocumentMetadata()
        files_to_process = doc_files

        if incremental and not force:
            print("ğŸ” ë³€ê²½ëœ íŒŒì¼ í™•ì¸ ì¤‘...")
            changed_files = []
            unchanged_files = []

            for doc_file in doc_files:
                if metadata_manager.is_file_changed(doc_file):
                    changed_files.append(doc_file)
                else:
                    unchanged_files.append(doc_file)

            files_to_process = changed_files

            print(f"   ğŸ“Š ì „ì²´ íŒŒì¼: {len(doc_files)}ê°œ")
            print(f"   ğŸ”„ ë³€ê²½ëœ íŒŒì¼: {len(changed_files)}ê°œ")
            print(f"   âœ… ë³€ê²½ ì—†ëŠ” íŒŒì¼: {len(unchanged_files)}ê°œ")

            if not changed_files:
                print("âœ… ë³€ê²½ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                return

            print("\në³€ê²½ëœ íŒŒì¼ ëª©ë¡:")
            for f in changed_files:
                relative_path = f.relative_to(rag_pipeline.documents_dir)
                print(f"  - {relative_path}")
        else:
            print(f"\në°œê²¬ëœ ë¬¸ì„œ: {len(doc_files)}ê°œ")
            for f in doc_files[:10]:  # Show first 10
                relative_path = f.relative_to(rag_pipeline.documents_dir)
                print(f"  - {relative_path}")
            if len(doc_files) > 10:
                print(f"  ... ë° {len(doc_files) - 10}ê°œ ë”")

        # ë°°ì¹˜ ì²˜ë¦¬
        print(f"\nğŸ”„ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {batch_size})")

        # For now, process all at once (could be enhanced for true batch processing)
        result = await rag_pipeline.load_documents()

        if result["success"]:
            print("\nâœ… ì—…ë¡œë“œ ì™„ë£Œ!")
            print(f"  - ì²˜ë¦¬ëœ íŒŒì¼: {len(result['processed_files'])}ê°œ")
            print(f"  - ìƒì„±ëœ ì²­í¬: {result['total_chunks']}ê°œ")

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if incremental:
                print("ğŸ“ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘...")
                for doc_file in files_to_process:
                    metadata_manager.update_file_metadata(doc_file)
                metadata_manager.save_metadata()
                print("   âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ë¨")

            # Show stats
            stats = rag_pipeline.get_collection_stats()
            print("\nğŸ“Š ë²¡í„° DB í†µê³„:")
            print(f"  - ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
            print(f"  - ì²­í¬ í¬ê¸°: {stats['chunk_size']}")
            print(f"  - ì²­í¬ ì¤‘ë³µ: {stats.get('chunk_overlap', 'N/A')}")
            print(f"  - ì˜ë¯¸ë¡ ì  ì²­í‚¹: {'í™œì„±í™”' if stats.get('use_semantic_chunking') else 'ë¹„í™œì„±í™”'}")
            print(f"  - ì„ë² ë”© ëª¨ë¸: {stats['embedding_model']}")

            if result.get("errors"):
                print(f"\nâš ï¸  ì˜¤ë¥˜ ë°œìƒ: {len(result['errors'])}ê°œ")
                for error in result["errors"][:3]:
                    print(f"  - {error}")
                if len(result["errors"]) > 3:
                    print(f"  ... ë° {len(result['errors']) - 3}ê°œ ë”")
        else:
            print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback

        traceback.print_exc()


async def test_rag_query():
    """Test RAG query"""
    try:
        # Ensure API key is set
        from app.core.config import settings

        if settings.llm_api_key:
            os.environ["OPENAI_API_KEY"] = settings.llm_api_key

        from app.rag.enhanced_rag import rag_pipeline

        print("\n\nğŸ” RAG í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬")
        print("-" * 50)

        # Test queries
        test_queries = [
            "í”„ë¡œì íŠ¸ì˜ ì£¼ìš” ëª©í‘œëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ì‚¬ìš©ëœ ê¸°ìˆ  ìŠ¤íƒì€ ë¬´ì—‡ì¸ê°€ìš”?",
        ]

        for query in test_queries[:1]:  # Test with first query
            print(f"\nğŸ“ ì§ˆë¬¸: {query}")

            try:
                result = await rag_pipeline.answer_with_confidence(query, k=3)

                if not result or not isinstance(result, dict):
                    print("\nâŒ RAG íŒŒì´í”„ë¼ì¸ì—ì„œ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    print(
                        "ğŸ’¡ ì›ì¸: ë²¡í„° DBê°€ ë¹„ì–´ìˆê±°ë‚˜ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                    print(
                        "ğŸ”§ í•´ê²°: python3 clear_and_reload_docs.py ë¥¼ ì‹¤í–‰í•˜ì—¬ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ì¸ë±ì‹±í•˜ì„¸ìš”."
                    )
                    continue

                print(f"\nğŸ’¬ ë‹µë³€: {result.get('answer', 'ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤')}")
                print(f"ğŸ¯ ì‹ ë¢°ë„: {result.get('confidence', 'UNKNOWN')}")
                print(f"ğŸ’¡ ê·¼ê±°: {result.get('reasoning', 'ê·¼ê±° ì •ë³´ ì—†ìŒ')}")

                sources = result.get("sources", [])
                if sources:
                    print("\nğŸ“š ì¶œì²˜:")
                    for source in sources:
                        print(f"  - {os.path.basename(source)}")
                else:
                    print("\nğŸ“š ì¶œì²˜: ì—†ìŒ (ë¬¸ì„œê°€ ì¸ë±ì‹±ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")

                if "search_metadata" in result:
                    search_metadata = result["search_metadata"]
                    print("\nğŸ” ê²€ìƒ‰ ë©”íƒ€ë°ì´í„°:")
                    print(
                        f"  - ì¬ì‘ì„±ëœ ì¿¼ë¦¬ ìˆ˜: {len(search_metadata.get('rewritten_queries', []))}"
                    )
                    print(
                        f"  - ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {search_metadata.get('total_results', 0)}"
                    )
                    if "error" in search_metadata:
                        print(f"  - ê²€ìƒ‰ ì˜¤ë¥˜: {search_metadata['error']}")

            except Exception as e:
                print("\nâŒ RAG í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ:")
                print(f"   ì˜¤ë¥˜ ë©”ì‹œì§€: {str(e)}")
                print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")

                # ì¼ë°˜ì ì¸ í•´ê²°ì±… ì œì‹œ
                print("\nğŸ”§ ë¬¸ì œ í•´ê²° ë°©ë²•:")
                print(
                    "  1. ë²¡í„° DB ì´ˆê¸°í™”: python3 clear_and_reload_docs.py --clear-only -y"
                )
                print("  2. ë¬¸ì„œ ì¬ì¸ë±ì‹±: python3 clear_and_reload_docs.py -y")
                print("  3. í™˜ê²½ ë³€ìˆ˜ í™•ì¸: LLM_API_KEY, OPENAI_API_KEY ì„¤ì •")

                # ë” ìì„¸í•œ ë””ë²„ê¹… ì •ë³´
                import traceback

                print("\nğŸ› ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
                traceback.print_exc()

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")


def parse_arguments():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±"""
    parser = argparse.ArgumentParser(
        description="MOJI RAG ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python upload_docs.py                           # ëª¨ë“  ë¬¸ì„œ ì—…ë¡œë“œ
  python upload_docs.py --folder policies/       # íŠ¹ì • í´ë”ë§Œ ì—…ë¡œë“œ
  python upload_docs.py --file guide.txt         # íŠ¹ì • íŒŒì¼ë§Œ ì—…ë¡œë“œ
  python upload_docs.py --incremental            # ì¦ë¶„ ì—…ë°ì´íŠ¸
  python upload_docs.py --batch-size 10          # ë°°ì¹˜ í¬ê¸° ì„¤ì •
  python upload_docs.py --incremental --force    # ê°•ì œ ì „ì²´ ì¬ì¸ë±ì‹±
        """,
    )

    parser.add_argument("--folder", type=str, help="íŠ¹ì • í´ë”ë§Œ ì¸ë±ì‹± (ì˜ˆ: policies/)")

    parser.add_argument("--file", type=str, help="íŠ¹ì • íŒŒì¼ë§Œ ì¸ë±ì‹± (ì˜ˆ: guide.txt)")

    parser.add_argument(
        "--incremental", action="store_true", help="ì¦ë¶„ ì—…ë°ì´íŠ¸ (ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬)"
    )

    parser.add_argument(
        "--batch-size", type=int, default=5, help="ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’: 5)"
    )

    parser.add_argument(
        "--force",
        action="store_true",
        help="ê°•ì œ ì‹¤í–‰ (ì¦ë¶„ ëª¨ë“œì—ì„œë„ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬)",
    )

    parser.add_argument(
        "--no-test", action="store_true", help="í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ"
    )

    return parser.parse_args()


async def main():
    """Main function"""
    args = parse_arguments()

    print("ğŸ¤– MOJI RAG ë¬¸ì„œ ì—…ë¡œë“œ ë„êµ¬")
    print("=" * 50)

    # ì„¤ì • ì •ë³´ ì¶œë ¥
    if args.folder:
        print(f"ğŸ“ ëŒ€ìƒ í´ë”: {args.folder}")
    elif args.file:
        print(f"ğŸ“„ ëŒ€ìƒ íŒŒì¼: {args.file}")
    else:
        print("ğŸ“ ëŒ€ìƒ: ì „ì²´ ë¬¸ì„œ í´ë”")

    if args.incremental:
        print("ğŸ”„ ëª¨ë“œ: ì¦ë¶„ ì—…ë°ì´íŠ¸")
        if args.force:
            print("âš¡ ê°•ì œ ëª¨ë“œ: ëª¨ë“  íŒŒì¼ ì¬ì²˜ë¦¬")
    else:
        print("ğŸ”„ ëª¨ë“œ: ì „ì²´ ì—…ë¡œë“œ")

    print(f"ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    print()

    # Upload documents
    await upload_documents(
        folder=args.folder,
        file_path=args.file,
        incremental=args.incremental,
        batch_size=args.batch_size,
        force=args.force,
    )

    # Test query (unless disabled)
    if not args.no_test:
        response = input("\n\nRAG í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() == "y":
            await test_rag_query()

    print("\n\nğŸ’¡ ì›¹ì±—ì—ì„œ RAG ì‚¬ìš©í•˜ê¸°:")
    print("1. ì„œë²„ ì‹¤í–‰: uvicorn app.main:app --reload")
    print("2. ì›¹ì±— ì ‘ì†: http://localhost:8000/api/v1/adapters/webchat/page")
    print("3. ëª…ë ¹ì–´:")
    print("   - RAG í† ê¸€ì„ ONìœ¼ë¡œ ì„¤ì •")
    print("   - ì¼ë°˜ ì§ˆë¬¸ ì…ë ¥")
    print("   - /rag <ì§ˆë¬¸> - RAG ì „ìš© ê²€ìƒ‰")
    print("   - /help - ë„ì›€ë§")
    print("   - /model - í˜„ì¬ ëª¨ë¸ ì •ë³´")


if __name__ == "__main__":
    asyncio.run(main())
